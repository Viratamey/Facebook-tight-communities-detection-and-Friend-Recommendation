{"cells":[{"cell_type":"code","source":["#import data set\nfb_edges_file_name = \"/FileStore/tables/dataset_facebook_combined.txt\"\nfb_edges_file = sc.textFile(fb_edges_file_name)\nprint fb_edges_file.take(88236)\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#retrieve vertex and edges from the data set\ndef get_user1_tuple(entry):\n  row = entry.split(' ')\n  return int(row[0])\n\ndef get_user2_tuple(entry):\n  row = entry.split(' ')\n  return int(row[1])\n\ndef get_edge_tuple(entry):\n  row = entry.split(' ')\n  return int(row[0]),int(row[1])"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql import Row\nuser1RDD = fb_edges_file.map(get_user1_tuple).distinct()\nuser1Count = user1RDD.count()\nprint (user1Count)\nprint 'Vertices: %s' % user1RDD.takeOrdered(5)\nuser2RDD = fb_edges_file.map(get_user2_tuple).cache().distinct()\nuser2Count = user1RDD.count()\nprint (user2Count)\nprint 'Vertices: %s' % user2RDD.takeOrdered(5)\nuser1Union2 = user1RDD.union(user2RDD)\nuserRDD = user1Union2.distinct()\nuserCount = userRDD.count()\nprint (userCount)\nprint 'Vertices: %s' % userRDD.takeOrdered(5)\nedgesRDD = fb_edges_file.map(get_edge_tuple).cache().distinct()\necount = edgesRDD.count()\nprint (ecount)\nprint 'edges: %s' % edgesRDD.take(10)\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#import igraph package \nfrom igraph import *\n\n#build igraph with users and edges from the dataset\nvertices = userRDD.collect()\nedges = edgesRDD.collect()\ng = Graph(vertex_attrs={\"label\":vertices}, edges=edges, directed=False)\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#overall dataset analysis on the built graph\n#check if graph is connected or not and graph is birectional and having all edges have equal weights\nprint g.is_connected(mode=STRONG)\nprint g.farthest_points(directed=False, unconn=True, weights=None)\nnetwork_diameter = g.diameter(directed=False, unconn=True, weights=None)\nprint network_diameter\nprint g.get_diameter(directed=False, unconn=True, weights=None)\nnetwork_betweenness = g.betweenness(vertices=None, directed=False, cutoff=None, weights=None, nobigint=True)\n#reduce method in apache spark to calculate sum of all vertices' betweenness\nmeannetwork_betweenness= reduce(lambda x, y: x + y, network_betweenness) / len(network_betweenness)\nprint meannetwork_betweenness"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#check degree distribution of the network\nnetwork_degree = g.degree()\nprint network_degree\n#reduce method in apache spark to calculate sum of all vertices' degrees\nmean_network_degree= reduce(lambda x, y: x + y, network_degree) / len(network_degree)\nprint mean_network_degree\nfrom operator import add\nnetwork_degreeRDD = sc.parallelize(network_degree)\ncounts = network_degreeRDD.map(lambda x: (x, 1)).reduceByKey(add)\noutput = counts.collect()\nfor (degree, count) in output:\n  print(\"%s %i\" % (degree, count))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#identify insignificant nodes\ninsignificant_users_list = []\nfor v in vertices:\n  friends_list = g.neighbors(vertex=v, mode=ALL)\n  if (len(friends_list) < 2):\n    insignificant_users_list.append(v)\nprint set(insignificant_users_list)\n\ninsignificant_users_degree_list=[]\nfor i in insignificant_users_list:\n  insignificant_users_degree_list.append(g.degree(i))\nprint  set(insignificant_users_degree_list)\n\n#remove island nodes from the graph \ng.delete_vertices(insignificant_users_list)\nnew_vertices = []\nnew_edges = []\n\nfor v in g.vs:\n    new_vertices.append(v[\"label\"])\nfor e in g.es:\n    new_edges.append(e.tuple) \nprint len(set(vertices))    \nprint len(set(insignificant_users_list))    \nprint len(set(new_vertices))  \n\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#identify significant nodes\nimportant_users_list = []\nimportant_users_degree_list = []\nfor v in g.vs:\n  v_degree = g.degree(v)\n  if(v_degree > 300): \n    important_users_list.append(v.index)\n    print v.index\n    important_users_degree_list.append(v_degree)\nprint set(important_users_list)\nmean_important_users_degree = reduce(lambda x, y: x + y, important_users_degree_list) / len(important_users_degree_list)\nprint mean_important_users_degree\n#sub graph focussing on node \"0\" which was identified as significant node\nnode0_friends_list = g.neighbors(vertex=0, mode=ALL)\nfreinds_of_friends = g.neighborhood(vertices=0, order=2, mode=ALL)\nprint len(node0_friends_list)\nprint len(freinds_of_friends)\n\nnode0_friends_list.append(0)\nnode0_alters = []\nuser0_graph = g.subgraph(node0_friends_list, implementation = \"auto\")\n\nfor e in user0_graph.es:\n    print e.tuple\n    node0_alters.append(e.tuple)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#identify cliques on the subgraph\ncliques_user0 = user0_graph.maximal_cliques(min =4 , max =10)\nprint cliques_user0"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#community detection with centrality based approach using edge betweeness\ncommunities = user0_graph.community_edge_betweenness(directed=False)\nclusters = communities.as_clustering()\nprint communities\nprint 'a'\nprint clusters.modularity\nprint clusters\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#community detection using Newman's leading eigenvector method\nclusters = user0_graph.community_leading_eigenvector()\nprint clusters.modularity\nprint clusters"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#community detection using the label propagation method of Raghavan et al\nclusters = user0_graph.community_label_propagation()\nprint clusters.modularity\nprint clusters"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#community detection using the multilevel algorithm of Blondel et al.\nmultilevelclusters = user0_graph.community_multilevel()\nprint clusters.modularity\nprint clusters"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#community detection using the spinglass community detection method of Reichardt & Bornholdt\nclusters = user0_graph.community_spinglass()\nprint clusters.modularity\nprint clusters"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#community detection using fast greedy algorithm\nfastGreedy = user0_graph.community_fastgreedy()\nFGcluster = fastGreedy.as_clustering()\nprint FGcluster.modularity\nprint FGcluster"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#community detection using walk trap algorithm\nwalkTrap = user0_graph.community_walktrap() \nWTcluster = walkTrap.as_clustering()\nprint WTcluster.modularity\nprint WTcluster"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#community detection using info map algorithm\ninfoMap = user0_graph.community_infomap()\nprint infoMap.modularity\nprint infoMap.as_cover()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#Part 2 - Friend Recommendation based on clusters detected\n#Extract tuples from dataset\ndef returnTuple(entry):\n  row = entry.split(' ')\n  return int(row[0]),int(row[1]),-1\n\negoRDD = fb_edges_file.map(returnTuple)\nprint egoRDD.take(600)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#Detect no. of mutual friends for any two of the nodes from the graph \nmutual_friends=[]\nallusersfriend_list =[]\nusers = userRDD.collect()\nfor j in range(len(users)):\n    toNodes1  = []\n    toNodes2  = []    \n    for row in egoRDD.collect():\n      if row[0]==users[j]:\n        toNodes1.append(row[1])\n      elif row[1]==users[j]:\n        toNodes2.append(row[0])\n    toNodes1RDD = sc.parallelize(toNodes1)\n    toNodes2RDD = sc.parallelize(toNodes2)\n    user_friendsRDD = toNodes1RDD.union(toNodes2RDD).distinct()\n    user_friends = user_friendsRDD.collect()\n    allusersfriend_list.append(user_friends)\n    \nallusersfriend_listRDD = sc.parallelize(allusersfriend_list)  \nfor i in range(0,len(users)):\n  for j in range(i+1,len(users)):\n    user1RDD = sc.parallelize(allusersfriend_list[users[i]])\n    user2RDD = sc.parallelize(allusersfriend_list[users[j]])\n    mutual_between_user1_and_user2 = user1RDD.intersection(user2RDD).distinct()\n    count = mutual_between_user1_and_user2.count()\n    mutual_friends.append([(users[i], users[j]), count])\n\nmutual_friendsRDD =sc.parallelize(mutual_friends)\nsortedRdd = mutual_friendsRDD.sortBy(lambda a: -a[1])\nprint sortedRdd.collect()\n    \n    "],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["allusersfriend_listRDD = allusersfriend_list.collect()  \nprint allusersfriend_list[users[1]].intersection(allusersfriend_list[users[2]]).distinct()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#Select one user for whom friend suggestion has to be made\nfromuser=115\n#Filter mutual friend list for the selected user\nsuggestions_115_1 = sortedRdd.filter(lambda x:x[0][0]==fromuser).map(lambda x:(x[0][1],x[1]))\nprint suggestions_115_1.collect()\nsuggestions_115_2 = sortedRdd.filter(lambda x:x[0][1]==fromuser).map(lambda x:(x[0][0],x[1]))\nprint suggestions_115_2.collect()\nsuggestions_115 = suggestions_115_1.union(suggestions_115_2)\nprint suggestions_115.collect()\nsuggestions_115_sorted = suggestions_115.sortBy(lambda x:-x[1])\nprint suggestions_115_sorted.collect()\nsuggestions_115_RDD = suggestions_115_sorted.map(lambda x:x[0])\nprint suggestions_115_RDD.collect()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["#Get all friends of user 115\nfriends_115_1= egoRDD.filter(lambda x:x[0]==fromuser).map(lambda x:x[1])\nfriends_115_2= egoRDD.filter(lambda x:x[1]==fromuser).map(lambda x:x[0])\nfriends_115 = friends_115_1.union(friends_115_2)\nprint friends_115.collect()\n#Get all non friends of user 115\nalready_friends = suggestions_115_RDD.intersection(friends_115)\nsuggestions = suggestions_115_RDD.subtract(already_friends)\nprint suggestions.collect()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["#Narrow down suggestion based on communities\n#Communities detected by fastgreedy is opted because of better modularity\nsuggestion_list = suggestions.collect()\ncommunity_based_suggestion=[]\nfor cluster_index in range(8):\n  for member in suggestion_list:\n    if member in multilevelclusters[cluster_index] and 115 in multilevelclusters[cluster_index]:\n      community_based_suggestion.append(member)\n\nprint community_based_suggestion"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"Facebook communities detection and friend recommendation","notebookId":3078469415237831},"nbformat":4,"nbformat_minor":0}
